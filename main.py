# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14dYVYb46RkV5--DmDifFKDbQ-7EJPKpL
"""

!pip install fastapi uvicorn openai google-generativeai anthropic nest-asyncio pyngrok googlesearch-python

!pip install textstat

!ngrok config add-authtoken 2svaI0QnyBwbOaTP1VNi1Weyob5_6ps7U6Pdshg3LC5tQGm6K

from fastapi import FastAPI
import uvicorn
import torch
from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer
from googlesearch import search
from fastapi.middleware.cors import CORSMiddleware

# Initialize FastAPI app
app = FastAPI()

# Enable CORS for frontend access
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ✅ Load AI Detection Model (RoBERTa-based)
ai_detector = pipeline("text-classification", model="roberta-base-openai-detector")

# ✅ Load GPT-2 Model for Humanization
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

@app.get("/")
def home():
    """Check if FastAPI is running."""
    return {"message": "FastAPI is running successfully!"}

# ✅ AI Content Detection Route
@app.post("/ai-detection")
async def detect_ai_content(data: dict):
    text = data.get("text", "")
    result = ai_detector(text)
    ai_score = result[0]["score"] * 100  # Convert to percentage
    return {"ai_score": round(ai_score, 2)}

# ✅ Humanization Route (With User-Selected Writing Styles)
@app.post("/humanize-text")
async def humanize_text(data: dict):
    text = data.get("text", "")
    style = data.get("style", "formal")  # Default to formal style

    # Adjust model temperature based on style
    temperature = 0.7 if style == "formal" else 1.0 if style == "creative" else 0.5

    input_ids = tokenizer.encode(text, return_tensors="pt")
    output = model.generate(input_ids, max_length=300, temperature=temperature)

    humanized_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return {"humanized_text": humanized_text}

# ✅ Plagiarism Checking with Google Search
@app.post("/plagiarism-check")
async def check_plagiarism(data: dict):
    text = data.get("text", "")
    search_query = f'"{text[:50]}"'  # Search for the first 50 characters
    results = [url for url in search(search_query, num_results=5)]
    return {"possible_matches": results}

# ✅ Deploy on Railway
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Commented out IPython magic to ensure Python compatibility.
# Install Git if not available
!apt-get install git

# Configure Git (Set Your GitHub Info)
!git config --global user.email "vijaysherr222@gmail.com"
!git config --global user.name "vijaysheru"

# Clone the new GitHub repository
!git clone https://github.com/vijaysheru/sherrAI

# Move your backend code to the cloned repo
!mv main.py ai-response-aggregator/

# Change directory to the repository
# %cd ai-response-aggregator

# Add the files to Git
!git add .

# Commit the changes
!git commit -m "Added backend code from Google Colab"

# Push changes to GitHub
!git push origin main

import os
print(os.listdir("/content"))